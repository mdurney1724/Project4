


# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf

#  Import and read the diabetes_data.csv.
diabtetes_df = pd.read_csv("Resources/diabetes_data.csv")
diabtetes_df.head()


# Determine the number of unique values in each column.
diabtetes_df.nunique()


# Split our preprocessed data into our features and target arrays
y = diabtetes_df['Diabetes_012'].values
X = diabtetes_df.drop(['Diabetes_012','MentHlth','PhysHlth'], axis=1).values

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)


# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)





# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
number_input_features = len(X_train[0])
hidden_nodes_layer1 = 8
hidden_nodes_layer2 = 5

nn = tf.keras.models.Sequential()

# First hidden layer
nn.add(
    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation="relu")
)

# Second hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation="relu"))

# Output layer
nn.add(tf.keras.layers.Dense(3, activation="softmax"))

# Check the structure of the model
nn.summary()


# Compile the model
nn.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])




# Train the model
fit_model = nn.fit(X_train_scaled,y_train,epochs=100)


# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


# Export our model to HDF5 file
nn.save('diabetes_prediction_model_r2.h5')


# List of parameters
parameters = [
    "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "HeartDiseaseorAttack", 
    "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "AnyHealthcare", "NoDocbcCost", 
    "GenHlth", "DiffWalk", "Sex", "Age", "Education", "Income"
]

# Initialize an empty list to store the user's inputs
user_inputs = []

# Loop through each parameter and prompt the user for input
for param in parameters:
    user_input = input(f"Please enter a value for {param}: ")
    # Convert the input to float or integer if necessary, or leave as string if appropriate
    # You might want to customize this depending on the data type of each input
    try:
        user_input = float(user_input) if '.' in user_input else int(user_input)
    except ValueError:
        pass  # If it's neither int nor float, leave it as string
    user_inputs.append(user_input)

# Print the array of inputs
print("User inputs:", user_inputs)


import numpy as np

# Load the pre-trained model
model = tf.keras.models.load_model('diabetes_prediction_model_r2.h5')

# Example: User inputs collected in the previous step
# Assuming the user_inputs array was generated as in the previous code
user_inputs = np.array(user_inputs).reshape(1, -1)  # Reshape the inputs to a 2D array, expected by the model

# Make a prediction using the loaded model
prediction = model.predict(user_inputs)

# Get the predicted class (use argmax for multi-class output or round for binary output)
predicted_class = np.argmax(prediction, axis=1)  # Assuming multi-class classification

# Print the result
if predicted_class[0] == 0:
    print("You are likely diabetes free.")
elif predicted_class[0] == 1:
    print("You are likely pre-diabetic.")
elif predicted_class[0] == 2:
    print("You are likely diabetic.")




